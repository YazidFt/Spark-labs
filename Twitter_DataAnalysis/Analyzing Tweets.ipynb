{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Data Analysis with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tweet consists of many data fields. [Here is an example](https://gist.github.com/arapat/03d02c9b327e6ff3f6c3c5c602eeaf8b). You can learn all about them in the Twitter API doc.\n",
    "\n",
    "* `created_at`: Posted time of this tweet (time zone is included)\n",
    "* `id_str`: Tweet ID - we recommend using `id_str` over using `id` as Tweet IDs, becauase `id` is an integer and may bring some overflow problems.\n",
    "* `text`: Tweet content\n",
    "* `user`: A JSON object for information about the author of the tweet\n",
    "    * `id_str`: User ID\n",
    "    * `name`: User name (may contain spaces)\n",
    "    * `screen_name`: User screen name (no spaces)\n",
    "* `retweeted_status`: A JSON object for information about the retweeted tweet (i.e. this tweet is not original but retweeteed some other tweet)\n",
    "    * All data fields of a tweet except `retweeted_status`\n",
    "* `entities`: A JSON object for all entities in this tweet\n",
    "    * `hashtags`: An array for all the hashtags that are mentioned in this tweet\n",
    "    * `urls`: An array for all the URLs that are mentioned in this tweet\n",
    "\n",
    "Tweets are collected using the [Twitter Streaming API](https://dev.twitter.com/streaming/overview).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class OutputLogger:\n",
    "    def __init__(self):\n",
    "        self.ans = {}\n",
    "\n",
    "    def append(self, key, value):\n",
    "        self.ans[key] = value\n",
    "\n",
    "my_output = OutputLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements: 2150\n"
     ]
    }
   ],
   "source": [
    "with open('./hw2-files.txt') as f:\n",
    "    file_path = [w.strip() for w in f.readlines() if w.strip()]\n",
    "\n",
    "rdd = sc.textFile(file_path[0])\n",
    "#rdd.cache()\n",
    "count = rdd.count()\n",
    "\n",
    "my_output.append(\"num-tweets\", count)\n",
    "print('Number of elements:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input is a String\n",
    "* Output is a JSON object if the tweet is valid and None if not valid\n",
    "* Output is a JSON object if the tweet is valid (valid if json string starts with \"created_at\")\n",
    "\n",
    "`We construct a pair RDD of (user_id, text)`: rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def safe_parse(raw_json):    \n",
    "    s = raw_json[2:12]\n",
    "    if(s == \"created_at\"):\n",
    "        json_obj = json.loads(raw_json)\n",
    "        return json_obj\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = rdd.collect()\n",
    "r = []\n",
    "\n",
    "for s in l:\n",
    "    obj = safe_parse(s)\n",
    "    if obj != None:\n",
    "        r.append((obj['user']['id_str'], obj['text'])) \n",
    "\n",
    "rdd2 = sc.parallelize(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique users is: 1748\n"
     ]
    }
   ],
   "source": [
    "users_count = rdd2.map(lambda p: p[0]).distinct().count()\n",
    "print('The number of unique users is:', users_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of posts from each user partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Pickle file `users-partition.pickle`, We have a dictionary which represents a partition over 452,743 Twitter users, `{user_id: partition_id}`. The users are partitioned into 7 groups. For example, if the dictionary is loaded into a variable named `partition`, the partition ID of the user `59458445` is `partition[\"59458445\"]`. These users are partitioned into 7 groups. The partition ID is an integer between 0-6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "objects = []\n",
    "\n",
    "with (open(\"users-partition2.pickle\", \"rb\")) as f:\n",
    "    while True:\n",
    "        try:\n",
    "            objects.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets per user partition\n",
    "\n",
    "* Next we will count the number of posts from each user partition, \n",
    "* Count the number of posts from group 0, 1, ..., 6 ( `We assign users who are not in any partition to the group 7`)\n",
    "\n",
    "The results of this step is constructed as a pair RDD `(group_id, count)` that is sorted by key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_post_count(counts):\n",
    "    for group_id, count in counts:\n",
    "        print('Group %d posted %d tweets' % (group_id, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 posted 87 tweets\n",
      "Group 1 posted 242 tweets\n",
      "Group 2 posted 41 tweets\n",
      "Group 3 posted 349 tweets\n",
      "Group 4 posted 101 tweets\n",
      "Group 5 posted 358 tweets\n",
      "Group 6 posted 434 tweets\n",
      "Group 7 posted 521 tweets\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "json_data = objects[0]\n",
    "#{user_id: partition_id, user_id2: partition_id2 ...} to array: [(user_id,partition_id), (user_id2, partition_id2)...]\n",
    "\n",
    "arr_id_partitions = []\n",
    "\n",
    "for k, v in json_data.items():\n",
    "    arr_id_partitions.append((k,v))\n",
    "    \n",
    "rdd3 = sc.parallelize(arr_id_partitions)\n",
    "\n",
    "\n",
    "#Construct DataFrames:\n",
    "\n",
    "# schema1 = StructType([StructField(\"Id\", StringType(), False), StructField(\"partition\", StringType(), False)])\n",
    "# schema2 = StructType([StructField(\"Id\", StringType(), False), StructField(\"text\", StringType(), False)])\n",
    "# df1 = sqlContext.createDataFrame(rdd3, schema1)\n",
    "# df2 = sqlContext.createDataFrame(rdd2, schema2)\n",
    "# df_joined = df1.join(df2, on = \"Id\", how = 'right')\n",
    "\n",
    "\n",
    "#Join Rdds:\n",
    "joined_rdd = rdd3.rightOuterJoin(rdd2)\n",
    "\n",
    "# users with (partition_id == None) assign 7\n",
    "interm_rdd = joined_rdd.map(lambda p: (7,p[1][1]) if(p[1][0] == None)  else (p[1][0],p[1][1]))\n",
    "\n",
    "# compute the countes:\n",
    "result_rdd = interm_rdd.map(lambda p: (p[0], 1)).reduceByKey(lambda p,q: p + q)\n",
    "counts_per_partition = result_rdd.collect()\n",
    "\n",
    "assert(type(counts_per_partition) is list and len(counts_per_partition) == 8 and len(counts_per_partition[0]) == 2)\n",
    "print_post_count(counts_per_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Tokens that are relatively popular in each user partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are going to find tokens that are relatively popular in each user partition.\n",
    "\n",
    "* Let $N_t^k$ be the number of mentions of the token $t$ in the user partition $k$ (the number of users who belongs the   $k'th$ partition and mention $t$). Let $N_t^{all} = \\sum_{i=0}^7 N_t^{i}$ be the number of total mentions of the token $t$.\n",
    "\n",
    "* We define the relative popularity of a token $t$ in a user partition $k$ as the log ratio between $N_t^k$ and $N_t^{all}$, i.e. \n",
    "\n",
    "\\begin{equation}\n",
    "p_t^k = \\log \\frac{N_t^k}{N_t^{all}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "tok = Tokenizer(preserve_case=False)\n",
    "\n",
    "def get_rel_popularity(c_k, c_all):\n",
    "    return log(1.0 * c_k / c_all) / log(2)\n",
    "\n",
    "\n",
    "def print_tokens(tokens, gid = None):\n",
    "    group_name = \"overall\"\n",
    "    if gid is not None:\n",
    "        group_name = \"group %d\" % gid\n",
    "    print('=' * 5 + ' ' + group_name + ' ' + '=' * 5)\n",
    "    for t, n in tokens:\n",
    "        print(\"%s\\t%.4f\" % (t, n))\n",
    "    print\n",
    "\n",
    "def print_tokens2(tokens, gid = None):\n",
    "    group_name = \"overall\"\n",
    "    if gid is not None:\n",
    "        group_name = \"group %d\" % gid\n",
    "    print('=' * 5 + ' ' + group_name + ' ' + '=' * 5)\n",
    "    for t, n in tokens:\n",
    "        print(\"%s\\t%d\" % (t, n))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize tweets\n",
    "\n",
    "* We will count the number of tokens we have in the datasets\n",
    "* And the number of mentions for each tokens regardless of specific user group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TokenizerKit import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 7677\n"
     ]
    }
   ],
   "source": [
    "num_of_tokens_rdd = interm_rdd.flatMap(lambda p: set(tok.tokenize(p[1])))\n",
    "num_of_tokens = len(set(num_of_tokens_rdd.collect()))\n",
    "\n",
    "print(\"Number of tokens:\", num_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token popularity\n",
    "\n",
    "In this section we will filter tokens that are mentioned by less than 100 users ( the top 20 most frequent tokens are showed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 51\n",
      "===== overall =====\n",
      ":\t1294\n",
      "rt\t1147\n",
      ".\t833\n",
      "the\t652\n",
      "trump\t623\n",
      "…\t602\n",
      "to\t543\n",
      ",\t530\n",
      "in\t440\n",
      "a\t414\n",
      "is\t399\n",
      "!\t324\n",
      "of\t308\n",
      "for\t297\n",
      "and\t278\n",
      "i\t229\n",
      "on\t227\n",
      "he\t198\n",
      "that\t197\n",
      "\"\t194\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#num_of_tokens_rdd = interm_rdd.flatMap(lambda p: [(it[0],it[1]) for it in Counter(list(tok.tokenize(p[1]))).items()])\n",
    "\n",
    "num_of_tokens_rdd = interm_rdd.flatMap(lambda p: [(t,1) for t in set(tok.tokenize(p[1]))])\n",
    "num_freq_tokens_rdd1 = num_of_tokens_rdd.reduceByKey(lambda x, y: x + y).filter(lambda p: p[1] > 99)\n",
    "num_freq_tokens = num_freq_tokens_rdd1.count()\n",
    "\n",
    "#TOP 20:\n",
    "top20 = sorted(num_freq_tokens_rdd1.collect(), key=lambda t: t[1], reverse=True)\n",
    "top20 = top20[:20]\n",
    "\n",
    "print(\"Number of tokens:\", num_freq_tokens)\n",
    "print_tokens2(top20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative popularity\n",
    "* The next code compute the relative popularity `in each user group` for all the previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== group 0 =====\n",
      "with\t-3.6668\n",
      "his\t-3.7279\n",
      "cruz\t-3.8074\n",
      "amp\t-3.9341\n",
      "on\t-4.0192\n",
      "this\t-4.0980\n",
      "to\t-4.1306\n",
      "&\t-4.1813\n",
      "https\t-4.1964\n",
      "what\t-4.2352\n",
      "===== group 1 =====\n",
      "sanders\t-2.2538\n",
      "hillary\t-2.2730\n",
      "’\t-2.3099\n",
      "gop\t-2.4195\n",
      "bernie\t-2.4964\n",
      "this\t-2.6386\n",
      "are\t-2.6439\n",
      "clinton\t-2.6439\n",
      "that\t-2.7152\n",
      "&\t-2.7408\n",
      "===== group 2 =====\n",
      "...\t-4.1964\n",
      "donald\t-4.2310\n",
      "with\t-4.4037\n",
      "gop\t-4.7415\n",
      "on\t-5.0192\n",
      "i\t-5.0318\n",
      "he\t-5.0444\n",
      "@berniesanders\t-5.1430\n",
      "https\t-5.1964\n",
      "what\t-5.2352\n",
      "===== group 3 =====\n",
      "@berniesanders\t-0.7737\n",
      "bernie\t-1.4964\n",
      "sanders\t-1.5619\n",
      "in\t-2.2578\n",
      "hillary\t-2.2730\n",
      "clinton\t-2.3219\n",
      "and\t-2.5644\n",
      "\"\t-2.5999\n",
      "will\t-2.6761\n",
      "...\t-2.6939\n",
      "===== group 4 =====\n",
      "@berniesanders\t-3.1430\n",
      "what\t-3.3607\n",
      "have\t-3.4983\n",
      "this\t-3.5131\n",
      "\"\t-3.5999\n",
      "bernie\t-3.6033\n",
      "?\t-3.6483\n",
      "vote\t-3.7004\n",
      "that\t-3.7152\n",
      "it\t-3.7249\n",
      "===== group 5 =====\n",
      "what\t-1.7758\n",
      "not\t-1.8461\n",
      "his\t-1.9730\n",
      "https\t-2.0265\n",
      "cruz\t-2.0849\n",
      "it\t-2.0875\n",
      "if\t-2.1346\n",
      "on\t-2.1541\n",
      "i\t-2.1668\n",
      "&\t-2.1813\n",
      "===== group 6 =====\n",
      "@realdonaldtrump\t-0.7758\n",
      "vote\t-1.3081\n",
      "will\t-1.4183\n",
      "have\t-1.4279\n",
      "!\t-1.4450\n",
      "trump\t-1.5896\n",
      "-\t-1.6815\n",
      "amp\t-1.7415\n",
      ";\t-1.7574\n",
      "cruz\t-1.7717\n",
      "===== group 7 =====\n",
      "donald\t-1.0971\n",
      "trump\t-1.7674\n",
      "clinton\t-1.7859\n",
      "if\t-1.8002\n",
      "bernie\t-1.8443\n",
      "’\t-1.8504\n",
      "sanders\t-1.8667\n",
      "of\t-1.9269\n",
      "be\t-1.9527\n",
      "with\t-1.9887\n"
     ]
    }
   ],
   "source": [
    "pre_join1 = interm_rdd.flatMap(lambda p: [((t,p[0]),1) for t in set(tok.tokenize(p[1]))])\n",
    "pre_join2 = pre_join1.reduceByKey(lambda x, y: x + y).map(lambda it: (it[0][0],(it[0][1],it[1])))\n",
    "\n",
    "jointed2 = pre_join2.join(num_freq_tokens_rdd1)\n",
    "\n",
    "answerRdd = jointed2.map(lambda p: (p[1][0][0],(p[0], get_rel_popularity(p[1][0][1],p[1][1])))).groupByKey()\n",
    "\n",
    "lists = answerRdd.map(lambda it: list(it[1])).collect()\n",
    "\n",
    "\n",
    "popular_words_in_each_group = [] #sorted by (popularity \"desc\" then alphabetically \"asc\" )\n",
    "\n",
    "for li in lists:\n",
    "    tmp = sorted(li, key=lambda t: (-t[1],t[0]))[:10]\n",
    "    popular_words_in_each_group.append(tmp)\n",
    "\n",
    "for k in range(8):\n",
    "    #Just the first 10 in each group\n",
    "    print_tokens(popular_words_in_each_group[k], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
